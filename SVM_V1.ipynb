{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f46043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM  best 2500 cu CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79630c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3150ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(train_data, test_data, type=None):\n",
    "    scaler = None\n",
    "    if type == 'standard':\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    elif type == 'l1':\n",
    "        scaler = preprocessing.Normalizer(norm='l1')\n",
    "\n",
    "    elif type == 'l2':\n",
    "        scaler = preprocessing.Normalizer(norm='l2')\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler.fit(train_data)\n",
    "        scaled_train_data = scaler.transform(train_data)\n",
    "        scaled_test_data = scaler.transform(test_data) \n",
    "        return (scaled_train_data, scaled_test_data)\n",
    "    else:\n",
    "        print(\"No scaling was performed. Raw data is returned.\")\n",
    "        return (train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead30748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    return (y_true == y_pred).mean()\n",
    "#train_labels = train_labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de58cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(data):\n",
    "    new_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        new_data.append(item[1])\n",
    "        \n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476f8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    new_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        new_data.append(item[1])\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd6b4bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "training_data = np.genfromtxt('data_aliens/train_samples.txt', dtype='str', comments='\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', delimiter='\\t', invalid_raise = False)\n",
    "training_labels = np.loadtxt('data_aliens/train_labels.txt', dtype='int')\n",
    "\n",
    "validation_data = np.genfromtxt('data_aliens/validation_samples.txt', dtype='str', comments='\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', delimiter='\\t', invalid_raise = False)\n",
    "validation_labels = np.loadtxt('data_aliens/validation_labels.txt', dtype='int')\n",
    "\n",
    "merged_data = np.concatenate((training_data, validation_data), axis=0)\n",
    "merged_labels = np.concatenate((training_labels, validation_labels), axis=0)\n",
    "\n",
    "x_train = get_sentence(training_data)\n",
    "y_train = get_labels(training_labels)\n",
    "\n",
    "x_valid = get_sentence(validation_data)\n",
    "y_valid = get_labels(validation_labels)\n",
    "\n",
    "x_merged = get_sentence(merged_data)\n",
    "y_merged = get_labels(merged_labels)\n",
    "\n",
    "print(validation_data.shape)\n",
    "\n",
    "# print(training_data)\n",
    "# print(get_labels(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7452b00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3000)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(lowercase = False, max_features=3000, strip_accents=None, min_df=1, stop_words=None)\n",
    "X_train_counts = count_vect.fit_transform(x_train)\n",
    "names = count_vect.get_feature_names_out()\n",
    "X_test_counts = count_vect.transform(x_valid)\n",
    "#test_names = count_vect.get_feature_names_out()\n",
    "\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "print(X_train_counts.toarray())\n",
    "\n",
    "train_data = X_train_counts.toarray()\n",
    "test_data = X_test_counts.toarray()\n",
    "\n",
    "#print(X_test_counts.shape)\n",
    "#count_vect.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "297d29f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4830a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame\n",
    "import pandas as pd\n",
    "\n",
    "def final_features(row, vect, names):\n",
    "    df = pd.DataFrame(data=vect,\n",
    "                  columns = names)\n",
    "    \n",
    "    txt = pd.DataFrame(row, columns = [\"data\"])\n",
    "    \n",
    "    vowels=['a','e','i','o','u']\n",
    "    \n",
    "    punc = ['!', \",\" ,\"\\'\" ,\";\" ,\"\\\"\", \".\", \"-\" ,\"?\"]\n",
    "    \n",
    "    def vowels_density(txt):\n",
    "        words = len(txt.split(\" \"))\n",
    "        chars = len(txt.replace(\" \", \"\"))\n",
    "        \n",
    "        nr_vowels = sum([1 for c in txt if c in vowels])\n",
    "        \n",
    "        return(nr_vowels / chars)\n",
    "    \n",
    "    def punc_density(txt):\n",
    "        words = len(txt.split(\" \"))\n",
    "        chars = len(txt.replace(\" \", \"\"))\n",
    "        \n",
    "        nr_punc = sum([1 for c in txt if c in punc])\n",
    "        \n",
    "        return(nr_punc / chars)\n",
    "    #print(\"denitate vocale: \", txt[\"data\"].apply(lambda txt: vowels_density(txt)))\n",
    "    \n",
    "    #df[\"num_question_marks\"] = txt[\"data\"].apply(lambda txt: txt.count(\"?\"))\n",
    "    df['punc_density'] = txt[\"data\"].apply(lambda txt: punc_density(txt))\n",
    "    df['vowels_density'] = txt[\"data\"].apply(lambda txt: vowels_density(txt))\n",
    "    df['num_unique_words'] = txt['data'].apply(lambda txt: len(set(w for w in txt.split())))\n",
    "    #df['capitals'] = txt['data'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    #print(df[\"num_question_marks\"])\n",
    "    print(\"DF\",df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca905b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.DataFrame(data=train_data,\n",
    "#                   columns = count_vect.get_feature_names_out())\n",
    "# print(\"DF\",df)\n",
    "\n",
    "\n",
    "\n",
    "# txt = pd.DataFrame(x_train, columns = [\"data\"])\n",
    "\n",
    "# # def counter(txt):\n",
    "# #     nr = 0\n",
    "# #     for i in range(0,len(txt)):\n",
    "# #         nr txt[i][\"data\"] == '*':\n",
    "# #             nr += 1\n",
    "# #     return nr\n",
    "# df[\"num_question_marks\"] = txt[\"data\"].apply(lambda txt: txt.count(\"?\"))\n",
    "# print(df[\"num_question_marks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92148b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF       00  000  0002  000g001  001  002  003  004  005  006  ...  țkK  țlrȚ  \\\n",
      "0      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "1      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "2      1    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "3      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "...   ..  ...   ...      ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   \n",
      "9995   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "9996   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "9997   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "9998   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "9999   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "\n",
      "      țo  țr  țz  țó  țăêvH  punc_density  vowels_density  num_unique_words  \n",
      "0      0   0   0   0      0      0.059471        0.066079                68  \n",
      "1      0   0   0   0      0      0.062201        0.047847                32  \n",
      "2      0   0   0   0      0      0.071429        0.037594                83  \n",
      "3      0   0   0   0      0      0.062718        0.062718                42  \n",
      "4      0   0   0   0      0      0.086207        0.034483                46  \n",
      "...   ..  ..  ..  ..    ...           ...             ...               ...  \n",
      "9995   0   0   0   0      0      0.043956        0.057692                60  \n",
      "9996   0   0   0   0      0      0.036496        0.029197                18  \n",
      "9997   0   0   0   0      0      0.064583        0.056250                69  \n",
      "9998   0   0   0   0      0      0.064407        0.054237                45  \n",
      "9999   0   0   0   0      0      0.056673        0.049360                76  \n",
      "\n",
      "[10000 rows x 3003 columns]\n",
      "DF       00  000  0002  000g001  001  002  003  004  005  006  ...  țkK  țlrȚ  \\\n",
      "0      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "1      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "2      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "3      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "...   ..  ...   ...      ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   \n",
      "4995   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4996   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4997   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4998   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4999   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "\n",
      "      țo  țr  țz  țó  țăêvH  punc_density  vowels_density  num_unique_words  \n",
      "0      0   0   0   0      0      0.080769        0.023077                40  \n",
      "1      0   0   0   0      0      0.039755        0.051988                53  \n",
      "2      0   0   0   0      0      0.024752        0.039604                39  \n",
      "3      0   0   0   0      0      0.026596        0.093085                66  \n",
      "4      0   0   0   0      0      0.104396        0.038462                35  \n",
      "...   ..  ..  ..  ..    ...           ...             ...               ...  \n",
      "4995   0   0   0   0      0      0.057034        0.068441                52  \n",
      "4996   0   0   0   0      0      0.050420        0.058824                66  \n",
      "4997   0   0   0   0      0      0.046512        0.055814                66  \n",
      "4998   0   0   0   0      0      0.065041        0.044715                44  \n",
      "4999   0   0   0   0      0      0.065657        0.053030                69  \n",
      "\n",
      "[5000 rows x 3003 columns]\n"
     ]
    }
   ],
   "source": [
    "# #valid\n",
    "\n",
    "# valid_df = pd.DataFrame(data=test_data,\n",
    "#                   columns = count_vect.get_feature_names_out())\n",
    "\n",
    "# txt = pd.DataFrame(x_valid, columns = [\"data\"])\n",
    "\n",
    "# valid_df[\"num_question_marks\"] = txt[\"data\"].apply(lambda txt: txt.count(\"?\"))\n",
    "# print(df[\"num_question_marks\"])\n",
    "\n",
    "train_df = final_features(x_train, train_data, names)\n",
    "\n",
    "valid_df = final_features(x_valid, test_data, names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea25d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizare a datelor\n",
    "scale_train, scale_validation = train_df, valid_df\n",
    "\n",
    "#scale_train, scale_validation = normalize_data(train_df, valid_df, \"l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ce368e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scale_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c42120f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5906\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = svm.SVC(C=3, kernel='linear')\n",
    "\n",
    "svm_classifier.fit(scale_train, y_train)\n",
    "predicted = svm_classifier.predict(scale_validation)\n",
    "\n",
    "print(accuracy_score(predicted, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb2edca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procces_merged(x_merged):\n",
    "    count_vect = CountVectorizer(lowercase = False, max_features=3000, strip_accents=None, min_df=1, stop_words=None)\n",
    "    x_merged_counts = count_vect.fit_transform(x_merged)\n",
    "    data = x_merged_counts.toarray()\n",
    "    \n",
    "    names = count_vect.get_feature_names_out()\n",
    "    \n",
    "    merged_df = final_features(x_merged, data, names)\n",
    "    \n",
    "    scale_merged, scale_merged = normalize_data(merged_df, merged_df, \"l2\")\n",
    "    \n",
    "    return scaled_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61deb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predicted)\n",
    "# print(predicted.shape)\n",
    "# print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bf68dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_prediction(test_data, predicted):\n",
    "    header = ['id', 'label']\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(0, len(test_data)):\n",
    "        id = int(test_data[i][0])\n",
    "        label = int(predicted[i])\n",
    "        \n",
    "        data.append([id, label])\n",
    "        \n",
    "    with open('submisie', 'w', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        for item in data:\n",
    "            writer.writerow(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fa73c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF       00  000  0002  000g001  001  002  003  004  005  006  ...  țkK  țlrȚ  \\\n",
      "0      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "1      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "2      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "3      0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4      0    0     0        0    0    0    0    0    1    0  ...    0     0   \n",
      "...   ..  ...   ...      ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   \n",
      "4995   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4996   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4997   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4998   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "4999   0    0     0        0    0    0    0    0    0    0  ...    0     0   \n",
      "\n",
      "      țo  țr  țz  țó  țăêvH  punc_density  vowels_density  num_unique_words  \n",
      "0      0   0   0   0      0      0.052764        0.032663                53  \n",
      "1      0   0   0   0      0      0.031496        0.062992                25  \n",
      "2      0   0   0   0      0      0.067532        0.041558                58  \n",
      "3      0   0   0   0      0      0.063492        0.058201                38  \n",
      "4      0   0   0   0      0      0.046036        0.040921                63  \n",
      "...   ..  ..  ..  ..    ...           ...             ...               ...  \n",
      "4995   0   0   0   0      0      0.064103        0.051282                32  \n",
      "4996   0   0   0   0      0      0.048718        0.058974                64  \n",
      "4997   0   0   0   0      0      0.019553        0.053073                54  \n",
      "4998   0   0   0   0      0      0.053265        0.042955                87  \n",
      "4999   0   0   0   0      0      0.030769        0.076923                11  \n",
      "\n",
      "[5000 rows x 3003 columns]\n"
     ]
    }
   ],
   "source": [
    "# Citire date test\n",
    "\n",
    "test_data_init = np.genfromtxt('data_aliens/test_samples.txt', dtype='str', comments='\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', delimiter='\\t', invalid_raise = False)\n",
    "\n",
    "x_test = get_sentence(test_data_init)\n",
    "\n",
    "X_test_counts = count_vect.transform(x_test)\n",
    "\n",
    "test_data = X_test_counts.toarray()\n",
    "\n",
    "test_df = final_features(x_test, test_data, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dccee710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale_test, scale_test = normalize_data(test_df, test_df, \"l1\")\n",
    "scale_test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c8d11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(lowercase = False, max_features=3000, strip_accents=None, min_df=1, stop_words=None)\n",
    "x_merged_counts = count_vect.fit_transform(x_merged)\n",
    "data = x_merged_counts.toarray()\n",
    "\n",
    "names = count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bccb6681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF        00  000  0001  0002  000g001  001  002  003  004  005  ...  țkK  țlrȚ  \\\n",
      "0       0    0     0     0        0    0    0    0    0    0  ...    0     0   \n",
      "1       0    0     0     0        0    0    0    0    0    0  ...    0     0   \n",
      "2       1    0     0     0        0    0    0    0    0    0  ...    0     0   \n",
      "3       0    0     0     0        0    0    0    0    0    0  ...    0     0   \n",
      "4       0    0     0     0        0    0    0    0    0    0  ...    0     0   \n",
      "...    ..  ...   ...   ...      ...  ...  ...  ...  ...  ...  ...  ...   ...   \n",
      "14995   0    0     0     0        0    0    0    0    0    0  ...    0     0   \n",
      "14996   0    0     0     0        0    0    0    0    0    0  ...    0     0   \n",
      "14997   0    0     0     0        0    0    0    0    0    0  ...    0     0   \n",
      "14998   0    0     0     0        0    0    0    0    0    0  ...    0     0   \n",
      "14999   0    0     0     0        0    0    0    0    0    0  ...    0     0   \n",
      "\n",
      "       țo  țr  țz  țó  țăêvH  punc_density  vowels_density  num_unique_words  \n",
      "0       0   0   0   0      0      0.059471        0.066079                68  \n",
      "1       0   0   0   0      0      0.062201        0.047847                32  \n",
      "2       0   0   0   0      0      0.071429        0.037594                83  \n",
      "3       0   0   0   0      0      0.062718        0.062718                42  \n",
      "4       0   0   0   0      0      0.086207        0.034483                46  \n",
      "...    ..  ..  ..  ..    ...           ...             ...               ...  \n",
      "14995   0   0   0   0      0      0.057034        0.068441                52  \n",
      "14996   0   0   0   0      0      0.050420        0.058824                66  \n",
      "14997   0   0   0   0      0      0.046512        0.055814                66  \n",
      "14998   0   0   0   0      0      0.065041        0.044715                44  \n",
      "14999   0   0   0   0      0      0.065657        0.053030                69  \n",
      "\n",
      "[15000 rows x 3003 columns]\n"
     ]
    }
   ],
   "source": [
    "merged_df = final_features(x_merged, data, names)\n",
    "    \n",
    "# scaled_merged, scaled_merged = normalize_data(merged_df, merged_df, \"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c941add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_merged, scaled_merged = normalize_data(merged_df, merged_df, \"l1\")\n",
    "scaled_merged = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df59ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# svm_classifier = svm.SVC(C=3, kernel='linear')\n",
    "# svm_classifier.fit(scaled_merged, y_merged)\n",
    "\n",
    "predicted = svm_classifier.predict(scale_test)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d7dd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_prediction(test_data_init, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c2a4c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_init.shape)\n",
    "print(predicted.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
